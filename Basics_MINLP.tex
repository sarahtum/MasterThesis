\documentclass{article}

%\usepackage[latin1]{inputenc}% erm\"oglich die direkte Eingabe der Umlaute 
%\usepackage[T1]{fontenc} % das Trennen der Umlaute
%\usepackage{ngerman} % hiermit werden deutsche Bezeichnungen genutzt und 
                     % die W\"orter werden anhand der neue Rechtschreibung 
		     % automatisch getrennt.  
\usepackage{amsmath}	% for formulas
\usepackage{amssymb} % for mathbb	
\usepackage[margin=1in]{geometry} % for definition of margin     
\usepackage{hyperref}
\usepackage{color}
\usepackage{cite}
\usepackage{graphicx}
\title{\textbf{Techniques for MINLP}}
%\author{}
%\date{\today}

\begin{document}

\maketitle

\section{Basic facts about MINLP}
\begin{itemize}
\item MINLP is NP-hard (clear as it includes MILP)
\item MINLP is undecidable (i.e. there are quadratically constrained IP for which no computing device can compute optimum for all instances of the class). But we're ok if $X$ is compact or all functions are convex.
\item Online Benchmark Library at \url{https://mintoc.de/index.php/Main_Page}, providing set of application problems to evaluate numerical optimization methods
\item MIOCPs: mixed-integer nonlinear optimization problems constrained by a system of ODEs
\item
\item convention here: $x$ continuous variable, $y$ integer variable
\end{itemize}

\section{Two fundamental concepts}
\subsection{Relaxation Enforcement}
\begin{itemize}
\item A relaxation is obtained by enlarging the feasible set of the MINLP, for example, by ignoring some constraints of the problem.
\item A relaxation is used to compute a lower bound. 
\item Together with upper bounds, which can be obtained from any feasible point (e.g. by solving NLP for fixed $x_I$), relaxations allow us to terminate the search for a solution whenever the lower bound is larger than the current upper bound.
\end{itemize}

\subsection{Constraint Enforcement}
\begin{itemize}
\item goal: exclude solutions that are feasible for the relaxation but not to the original MINLP
\item may be accomplished by 
\begin{itemize}
\item refining or tightening the relaxation, often by adding valid inequalities ("cut"), or by 
\item branching, where the relaxation is divided into two or more separate problems, managed in a search tree
\item spatial branching: branch on continous variable and split domain in two parts
\end{itemize}
\item 
\end{itemize} 
 
\section{Relaxations}
\begin{itemize}
\item Relaxing integrality (yielding a \textcolor{blue}{\textit{nonlinear relaxation}}), used in Branch-and-Bound\\
\textbf{Note:} An NLP can also be obtained as an \textit{restriction} by fixing $y=y^k$

\item Relaxing convex constraints with a set of supporting hyperplanes (obtained by Taylor, yielding a \textcolor{blue}{\textit{polyhedral relaxation}}), used in outer approximation methods
\item Relaxing non-convex constraints (using convex underestimators), used in spatial Branch-and-Bound
\end{itemize}

\section{Solving Algorithms}
\begin{itemize}
\item better use active-set algorithms (as SQP) than Interior-Point methods
\item Sebastian: 
\begin{itemize}
\item if integer structure not too complex, rather use \textit{outer approximation}
\item if nonlinear structure not too complex, rather use \textit{Branch-and-Bound}
\item outer approximation: "try to solve MILP as NLP as well"
\item Branch-and-Bound: "try to solve NLP as MILP as well"
\end{itemize}
\item lack of hot-starts affects both active-set and interior-point methods
\item distinguish two broad classes of methods: 
\begin{itemize}
\item single-tree (like Branch-and-Bound, Branch-and-Cut) 
\item multi-tree (like outer approximation, Benders decomposition)
\end{itemize}
\item following overview roughly following \cite{d2013mixed}
\end{itemize}

\subsection{For convex MINLPs}
\subsubsection{Nonlinear Branch-and-Bound}
\begin{itemize}
\item compute lower bounds for NLP-subproblems (obtained by branching) by solving their continuous relaxations
\item given fractional value $y_j^*$ in solution $(x^*,y^*)$ of continuous relaxation divide into subproblems by adding constraint $y_j \leq \left \lfloor{y_j^*}\right \rfloor$ or $y_j \geq \left \lceil{y_j^*}\right \rceil$ 
\item iterate until solution of continuous relaxation of subproblem is integer feasible or infeasible or lower bound is not smaller than current best solution
\item requires solution of large number of NLP
\item nearly impossible to effectively hot-start NLP solver (which is very different to ILP)
\item main reason for this: factors are outdated as soon as a step is taken because Hessian and Jacobian matrices are nonlinear and non constant
\end{itemize}

\subsubsection{Outer-approximation}
\begin{itemize}
\item originally developed (apparently?) by Duran/Grossmann (1986) and Fletcher/Leyffer (1994)
\item idea: avoid huge number of NLPs, instead use available well-advanced MILP solvers
\item solve \emph{alternating} finite sequence of NLP subproblems (for obtaining feasible solutions that act as upper bounds, around which we linearize and modify the MILP)
\item and relaxed (linearized) versions of a MILP master program (solutions act as lower bounds)
\item assumptions: linearity of integer (or discrete) variables, convexity of nonlinear functions involving continuous variables
\item \textbf{Upper Bounding NLP Subproblem:} fix $y=y^k$ and solve non-linear problem (giving an upper bounding solution $x^k$)
	\begin{itemize}
	\item if subproblem is not feasible, a \emph{feasibility NLP subproblem} is solved and its optimal solution is used instead
	\end{itemize}
\item \textbf{MILP Master Problem:} 
	\begin{itemize}
	\item idea: develop equivalent linear representatiton (i.e. MILP) of original MINLP
	\item do so by linearizing MINLP around $x=x^k$ (the problem now is a relaxation of original MINLP, giving a lower bound)
	\item or - more general - by replacing the nonlinear functions by polyhedral outer approximations
	 \item note: by convexity this OA cut does not cut off any solutions
	\end{itemize}
\item done when upper and lower bound are same, otherwise update new $y^k$ as new fixed value of $y$ in next NLP subproblem
\end{itemize}
See figure \ref{fig:OA_FlowChart} for a schematic overview

\begin{figure}[htb]
\centering
\includegraphics[scale=1.2]{OA_FlowChart.png}
\caption{Schematic Overview of Outer Approximation Algorithm by Grossmann and Duran}
\label{fig:OA_FlowChart}
\end{figure}

\noindent
See \cite{duran1986outer} of Duran and Grossmann (1987) for an overview. See \href{https://optimization.mccormick.northwestern.edu/index.php/Outer-approximation_(OA)}{here} for a numerical example.

\subsubsection{Generalized Benders decomposition}
from \cite{sager2005numerical}, p. 60
\begin{itemize}
\item similar to outer approximation but using another master program, including the Langrangian
\item less constriants and variables, but weaker formulation that outer approximation
\item hardly used anymore today
\end{itemize}

\subsubsection{Extended Cutting Plane Method}
see e.g. practical overview of D'Ambrosio \cite{d2013mixed}
\subsubsection{LP/NLP based Branch-and-Bound}
see e.g. practical overview of D'Ambrosio \cite{d2013mixed}\\
or see section \ref{sec:filmint} about \texttt{filMINT} which implements this approach
\subsubsection{Hybrid Algorithms}
\begin{itemize}
\item combine BB and OA methods
\item e.g. algorithm used in \texttt{BONMIN} solver
\end{itemize}
\subsection{For non-convex MINLPs}
\begin{itemize}
\item methods that are exact for convex MINLP work as heuristics
\item problems with non-convexity: 
	\begin{itemize}
	\item NLP subproblems have many local minima, solvers do not guarantee global optimality ($\rightarrow$ problem with BB-based approaches)
	\item linearization cuts are in general not valid for non-convex constraints ($\rightarrow$ problem with OA-based approaches)
	\end{itemize}
\end{itemize}

\subsubsection{Use of convex envelopes or under-estimators}
see \cite{d2013mixed}
\begin{itemize}
\item add auxiliary variables "linearizing" non-linearities of original constraints. 
\item complex nonlinearities are subdivided into simpler ones, at cost of additional variables and constraints
\item for these simpler ones use simple known envelopes
\end{itemize}
\subsubsection{Spatial branch-and-bound}
\begin{itemize}
\item algorithm for non-convex MINLPs
\item underestimation of objective, overestimation of feasible set by appropriate convex under-estimators
\end{itemize}


\section{MINLP Modeling Practices}
see \cite{Belotti13}, page 13

\begin{itemize}
\item prefer linear over convex over nonconvex formulations
\item use convexification of binary quadratic programs (via smallest eigenvalue)
\item exploit low-rank Hessians (replace dense Hessian $W$ by $W = Z^T R^{-1} Z$ with $Z$ sparse, $R$ covariance matrix)
\item linearize of constraints, e.g. linearization of $x_1x_2$ for $x_2\in\{0,1\}$
\item avoid undefined nonlinear expressions
\item never model on/off constraints by multiplying by a binary variable
\end{itemize}

\noindent
see \cite{sager2005numerical}, page 53
\begin{itemize}
\item (geometrically motivated) reformulations for integer constraints
\item 
\end{itemize}

\noindent
see \cite{raman1994modelling}, 
a framework helping to generate effective models
\section{MINLP solvers}
\subsection{for convex MINLPs}
\subsubsection{DICOPT}
see practical overview \cite{d2013mixed}
\begin{itemize}
\item \textbf{DI}screte and \textbf{C}ontinupus \textbf{OPT}imizer
\item solves MINLP problems where nonlinearities only involve continuous variables, binary and integer variables only in linear functions
\item implements an OA algorithm (plus equality relaxation, augmented penalty)
\end{itemize}

\subsubsection{BONMIN}
see Bonami paper \cite{bonami2008algorithmic}
\begin{itemize}
\item introduces algorithmic framework BONMIN which is exact for convex MINLP, can be used as heuristic for non-convex MINLP
\item use flexible branch-and-cut-scheme, \textbf{combination of BB and OA}
	\begin{itemize}
	\item uses outer approximations \textit{and} subproblem relaxations (relaxing integrality) to compute lower bounds
	\item uses restrictions of integer variables $x\in X \cap \mathbb{Z}^n$ to $x\in \bar{X} \cap \mathbb{Z}^n$ to compute upper bounds
	\item when \emph{only subproblem relaxations} used to compute lower bounds (as solution of NLP) \\
	$\rightarrow$ classical branch-and-bound
	\item when \textit{only outer approximations} used to compute lower bounds (as solution of MILP) \\
	$\rightarrow$ classical OA algorithm
	\end{itemize}
\item implementation:
	\begin{itemize}
	\item \texttt{B-BB}: NLP Branch-and-Bound framework (based on interior-point NLP solver Ipopt)
	\item \texttt{B-OA}: OA framework based on Ipopt, LP solver Clp
	\item \texttt{B-Hyb}: hybrid algorithm obtaining either \texttt{B-BB} or \texttt{B-OA} or anything in between
	\end{itemize}
\item theoretical result: If $T=\{(x^1,y^1), ... ,(x^K,y^K)\}$ any set of points containing suitable points with KKT conditions, $P^{OA}(T)$ OA-relaxation around these points, then $P$ and $P^{OA}(T)$ are equivalent (having same optimal value, corresponding optimal solutions)
\end{itemize}
practical overview \cite{d2013mixed}:
\begin{itemize}
\item to solve heuristically a non-convex problem: only use \texttt{B-BB}
\item several options provided to treat non-convex problems
\end{itemize}

\subsubsection{filMINT}
\label{sec:filmint}
see filMINT paper \cite{abhishek2010filmint}
\begin{itemize}
\item solver for convex MINLPs at cost which is a small multiple of cost of a comparable MILP
\item avoids complete re-solution of master MILP by adding new linearizations at open nodes of branch-and-bound tree whenever integer solution is found
\item FilMINT combines MINTO branch-and-cut framework for MILP (allows to use cutting planes, primal heuristics etc.)  with filterSQP for NLPs (active set solver allowing warm-starting, taking advantage of good initial primal and dual iterates)
\item algorithm used: \textcolor{blue}{LP/NLP-based branch-and-bound} (LP/NLP-BB) \cite{quesada1992lp}
\begin{itemize}
\item similar to outer-approximation, but:
\item instead of solving alternating sequence of MILP master problems, NLP subproblems: interrupt solution of MILP master whenever new integer assignment is found, then solve a NLP subproblem
\item (as usual:) solution of this subproblem provides new outer approximation added to master MILP, solving updated MILP master continues
\item thus: only single branch-and-cut tree is created and searched
\end{itemize}
\end{itemize}

\subsection{further solvers}
all mentioned in practical overview \cite{d2013mixed}
\begin{itemize}
\item LAGO (LAgrangian Gloabl Optimizer, branch-and-cut)
\item LINGO and LINDOGlobal (branch-and-cut, can be turned into heuristic using multistart techniques for NLPs)
\item MINLPBB (branch-and-bound)
\item MINOPT (generalized Bender decomposition, outer approximation)
\item MINOTAUR (nonlinear branch-and-bound for convex MINLPs)
\item SBB (branch-and-bound, exact for convex, limit fathoming of infeasible nodes in non-convex case)
\item SCIP (global optimality for convex and non-convex, spatial branch-and-bound)
\end{itemize}

\subsection{for non-convex MINLPs}
\subsubsection{BARON}
see practical overview \cite{d2013mixed}
\begin{itemize}
\item solves non-convex MINLPs to global optimality
\item spatial branch-and-bound
\item one of the most effective solvers
\end{itemize}

\subsubsection{COUENNE}
\begin{itemize}
\item solves non-convex MINLPs to global optimality
\item \textbf{C}onvex \textbf{O}ver and \textbf{U}nder \textbf{EN}velopoes for \textbf{N}onlinear \textbf{E}stimation
\item spatial branch-and-bound
\end{itemize}

\bibliography{../library}{}
\bibliographystyle{plain}
\end{document}