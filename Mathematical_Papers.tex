\documentclass{article}
%\usepackage[latin1]{inputenc}% erm\"oglich die direkte Eingabe der Umlaute 
%\usepackage[T1]{fontenc} % das Trennen der Umlaute
%\usepackage{ngerman} % hiermit werden deutsche Bezeichnungen genutzt und 
                     % die W\"orter werden anhand der neue Rechtschreibung 
		     % automatisch getrennt.  
\usepackage{amsmath}	% for formulas
\usepackage{amssymb} % for mathbb	
\usepackage[margin=1in]{geometry} % for definition of margin     
\usepackage{hyperref} % for \ref, \eqref, \url(?)
\usepackage[dvipsnames]{xcolor} % for textcolors
\usepackage{cite} % for citations
\usepackage{verbatim} % for comment environment
\title{\textbf{Overview over Mathematical MINLP Papers}}
%\author{}
\date{}
\begin{document}

\maketitle
\tableofcontents

\newpage

\section{TODO}
\subsection{Leute}
\begin{itemize}
\item Prof. Sebastian Sager (Uni Magdeburg)
\item Christian Kirches
\item Bock-Gruppe Heidelberg
\item oft gelesen: Grossmann
\end{itemize}
\subsection{Themen}
\begin{itemize}
%\item Outer Approximation
%\item outer relaxation (Sebastian): meint wahrscheinlich: relaxation using outer approximation (i.e. in relaxed master problem relaxation is obtained by linearization around outer approximation points; in contrast to e.g. relaxing integrality constraints)
\item outer convexification
%\item hybrid methods: meint wahrscheinlich: methods dealing with both integer and continous variables
\end{itemize}
\subsection{Paper}
\begin{itemize}
%\item literaturliste von \cite{burer2012non}
\item Sager PhD-Thesis
\item Kirches PhD-Thesis
\item Kirches, Bock, Leyffer: Modeling Mixed-Integer Constrained
Optimal Control Problems in AMPL
\item Kirches, Bock, Sager: Mixed-integer NMPC for predictive cruise control of heavy-duty trucks
\item Ruiz, Grossmann: Global Optimization of Non-convex Generalized
Disjunctive Programs: A Review on Relaxations and Solution Methods (maybe similar methods can be used for non-convex MINLPs?)
\item Grossmann: Cutting planes for improved global logic-based outer approximation
for the synthesis of process networks (similar to non-convex outer approximation for MINLPs of Kesavan et al, but for non-convex generalized disjunctive programs instead)
\end{itemize}
\subsection{Fragen an Sebastian}
\begin{itemize}
\item Sager: methods for MINLP that come from MIOCPs (mixed integer optimal control problems) by "first discretize, then optimize". Therefore large number of integer variables s.t. standard techniques like nonlinear Branch-and-Bound and Outer Approximation fail. This is not what we are planning to do?!
\item MIOCPs? Was bedeutet optimal control genau? We'll have robot dynamics instead of ODE. Trotzdem MIOCP-Paper, nicht lieber allgemeines MINLP paper?
\item our problem: separable functions? Can be made separable by factorization? Zu speziell?
\item problem size?
\item Modell soll gesamtes Problem (inklusive Dynamik) beschreiben?!
\item logical statt integer? disjunctive programming?
\end{itemize}
\newpage
\section{Branch-and-Bound based approaches}
\subsection{Combinatorial integral approximation [2010, Sager, Jung, Kirches]}
see \cite{sager2011combinatorial}\\
\textcolor{orange}{UNCLEAR} whether convex or non-convex (pretend non-convex)
\begin{itemize}
\item focus on MINLP arising from "first discretize, then optimize" of MIOCPs, focus on combinatorial constraints
\item MIOCP: one control binary, can only change values on a prefixed time grid, minimize Mayer term
\item decomposition of MINLP into NLP and MILP (both can be solved comparatively efficiently)
\item discuss bounds for gap between MILP ad MINLP solution
\item approximation results:
	\begin{itemize}
	\item theorem stating how difference of solutions to initial value problem depends on integrated difference between control functions and difference between initial values
	\item estimate on integral over difference between control functions
	\item if discretization grid is fine enough, no integer gap exists
	\end{itemize}
\item new method:
	\begin{itemize}
	\item based on combinatorial approximation of integral over control deviations
	\item propose tailored, structure exploiting Branch and Bound strategy for MILP
	\end{itemize}	 
\end{itemize}
\subsection{The Lagrangian Relaxation for the Combinatorial Integral Approximation Problem [2012, Jung, Sager]}
\begin{itemize}
\item continues previous paper
\item study properties of Lagrangian of MILP
\item prove polynomial runtime in special cases
\end{itemize}
\subsection{Solving Mixed-Integer Nonlinear Programs by QP-Diving [2012, Mahajan, Leyffer, Kirches]}
see \cite{mahajan2012solving}\\
\textcolor{red}{CONVEX}
\begin{itemize}
\item new variant of Branch-and-Bound
\item main motivation: lack of hot-start capabilities in nonlinear solvers (a "hot-start" being a sikve that sodes not require any factorizations but takes advantage of factorizations of parent node and rank-one updates)
\item rather than solving expensive NLP at each node of Branch-and-Bound tree, solve quadratic approximation (QP approximation) at every node
\item use QP solves that can be hot-started
\item global convergence properties for convex MINLPs (by adjusting BB rules in a suitable manner)
\item implemented in \texttt{MINOTAUR}, a toolkit written in C++ for solving MINLP problems
\end{itemize}

\subsection{A Lagrangean based Branch-and-Cut algorithm for global optimization of nonconvex MINLPs with decomposable structures [Grossmann, Karuppiah, 2008]}
see \cite{karuppiah2008lagrangean}\\
\textcolor{green}{NON-CONVEX}
\begin{itemize}
\item decomposable structures often arise in two-stage stochastic programming problems
\item decomposable here means: objective function can be written as sum of simpler functions
\item propose branch-and-cut approach
	\begin{itemize}
	\item usually: lower bounds obtained by solving relaxations constructed by convexifying
	\item often weak relaxations, resulting in weak bounds
	\item basic idea: cutting plane technique based on Lagrangean decomposition to produce tight relaxations
		\begin{itemize}
		\item solve subproblems obtained by decomposing ($P$) following Lagrangean decomposition
		\item use global subproblem solutions to generate (valid, tightening) cuts, add them to convex relaxation (master problem ($R$) of original model
		\item upper bounds obtained by heuristic
		\item termination: fathom node in BB tree if lower bound exceeds overall upper bound, approximation gap below tolerance, upper bound of $-\infty$
		\item branch domain of node
		\end{itemize}
	\end{itemize}
\item convergence result (informal argument)
\item two real-world examples: solution time reduced by more than order of magnitude compared to conventional BB scheme (BARON), but not established for large-scale problems
\end{itemize}

\subsection{Integrating SQP and Branch-and-Bound for Mixed Integer Nonlinear Programming [Leyffer, 2001]}
see \cite{leyffer2001integrating}\\
\textcolor{red}{CONVEX}
\begin{itemize}
\item in contrast to "decomposition" approaches (like BB, OA) separating nonlinear part from integer part, Leyffer suggests integrated approach
\item BB approach, but: NLP problem at each node does not need to be solved to optimality
\item instead: branching is allowed after each iteration of NLP solver (here: Sequential Quadratic Programming solver), i.e. branch early\\
$\rightarrow$ non-linear part of MINLP is thus solved whilst searching the tree, i.e. tree search and iterative solution of NLP are interlaced
\item idea first presented in \cite{borchers1994improved} by Borchers and Mitchell, improvements in this paper:
	\begin{itemize}
	\item prove that no need for computing lower bounds
	\item use own SQP solver
	\item new heuristic when to branch early
	\end{itemize}
\item idea similar to Quesada and Grossmann \cite{quesada1992lp} (LP/NLP based BB) but they solve NLP problems at some nodes, new solver usually solves only one QP problem at each node
\item algorithm \emph{only for convex} MINLP, only heuristic for non-convex
\item convergence proof, even global when slight changes (use trust-region method to enforce global convergence for SQP)
\item heuristic for non-convex MINLP: nodes no longer fathomed if QP problem is infeasible, instead solve feasibility problem until feasible QP problem is encountered
\item improves classical BB by factor up to 3
\end{itemize}

\newpage
\section{Outer-approximation based approaches}
\subsection{FilMINT: An Outer-Approximation-Based Solver for Nonlinear Mixed Integer Programs [Abhishek, Leyffer, Linderoth, 2008]}
\textcolor{red}{CONVEX}
\begin{itemize}
\item solver for convex MINLPs at cost which is a small multiple of cost of a comparable MILP, implementing linearization-based algorithm
\item avoids complete re-solution of master MILP by adding new linearizations at open nodes of branch-and-bound tree whenever integer solution is found
\item FilMINT combines MINTO branch-and-cut framework for MILP (allows to use cutting planes, primal heuristics etc.)  with filterSQP for NLPs (active set solver allowing warm-starting, taking advantage of good initial primal and dual iterates)
\item algorithm used: \textcolor{blue}{LP/NLP-based branch-and-bound} (LP/NLP-BB) \cite{quesada1992lp}
\begin{itemize}
\item similar to outer-approximation, but:
\item instead of solving alternating sequence of MILP master problems, NLP subproblems: interrupt solution of MILP master whenever new integer assignment is found, then solve a NLP subproblem
\item (as usual:) solution of this subproblem provides new outer approximation added to master MILP, solving updated MILP master continues
\item thus: only single branch-and-cut tree is created and searched
\end{itemize}
\end{itemize}

\subsection{Outer approximation algorithms for separable nonconvex mixed-integer nonlinear programs [Kesavan, Allgor, Gatzke, Barton, 2004]}
\label{sec:OA_nonconvex}
see \cite{kesavan2004outer}\\
\textcolor{green}{NONCONVEX, promising}\\
\textcolor{red}{old}\\
\textbf{What?} 
\begin{itemize}
\item  decomposition approach consisting of solving alternating sequence of relaxed master MILPs and two NLPs (trying to extend OA to non-convex case)
\item use Primal Bounding Problems, i.e. a convex NLP solved at each iteration to derive valid outer approximations of nonconvex functions in continous space
\item convergence and optimality properties proven
\item for non-convex MINLPs: recently a couple of algorithms based ond branch-and-bound strategies have been developed (e.g. branch-and-reduce, $\alpha$BB, spatial BB...). All based on solving lower bounding relaxed master problem, upper bounding primal problem at each node of BB tree. Only differ in formulation of master and primal problem and heuristic braching strategy.
\item this paper: these convex under-estimators employed by BB-approaches may also be exploited to develop decomposition algorithms for non-convex MINLPs
\item subproblems used in algorithm:	
	\begin{itemize}
	\item \textbf{Lower Bounding Convex MINLP ($P_1$}): solution yields lower bound to global solution of original problem ($P$)
		\begin{itemize}
		\item \textcolor{blue}{convexify} and underestimate non-convex functions in continuous variables (remember that ($P$) is separable). The closer this under-estimator to convex envelope, the tighter lower bound
		\item this modified problem ($P_1$) contains feasible set of ($P$) s.t. solution represents valid lower bound
		\end{itemize}
	\item \textbf{Master Problem ($M$)}: MILP, solution yields lower bound to global solution of ($P$)
		\begin{itemize}
		\item ($P_1$) is convex MINLP s.t. \textcolor{blue}{outer approximation} can be used to derive equivalent MILP ($M$)
		\item as ($P_1$) and ($M$) are equivalent, solution of ($M$) yields lower bound to original problem ($P$)
		\end{itemize}
	\item \textbf{Relaxed Master Problem ($M^k$)}: MILP, solution is lower bound on subset not yet explored
		\begin{itemize}
		\item similar to original OA algorithm relaxations of $M$ are solved at each iteration 
		\item integer cuts excluding previously examined integer relalizations are added to Relaxed Master problem
		\item solution of relaxed master problem yields new integer realization, iteration is repeated
		\end{itemize}
	\item \textbf{Primal Problem (NLP($y^j$))}: nonconvex NLP obtained by \textcolor{blue}{fixing binary variables ($y=y^j$) in ($P$)}. Any feasible solution yields upper bound to ($P$)
	\item \textbf{Primal Bounding Problem (NLPB($y^j$)}): convex NLP
		\begin{itemize}
		\item obtained by \textcolor{blue}{fixing $y=y^j$ in ($P_1$)}
		\item for fixed realization of $y$ the feasible set of (NLPB($y^j$)) overestimates the feasible set of (NLP($y^j$)) and underestimates the objective function of (NLP($y^j$))
		\item solution is lower bound to solution of (NLP($y^j$))
		\item solution tighter lower bound than the bound provided by relaxed master problem generating $y^j$
		\end{itemize}
	\end{itemize}
\item additional assumptions on ($P$) are discussed (twice differentiable)
\item Burer (\cite{burer2012non}) claims that authors assume that $h^j(y)$ are linear and $y$ is binary???
\item algorithms
	\begin{itemize}
	\item one algorithm finding global solution of non-convex MINLP
	\item one algorithm yielding rigorous bound on global solution
	\end{itemize}
\item theoretical and numerical properties
	\begin{itemize}
	\item convergence properties of algorithms (termination in finite number of steps)
	\item experimental results suggest: this decomposition strategy is more efficient (w.r.t. CPU time) when compared to currently available branch-and-bound algorithms for non-convex MINLPs
	\end{itemize}
\end{itemize}

\subsection{An Outer-Inner Approximation for separable MINLPs [Hijazi, Bonami, Ouorou, 2010]}
see \cite{hijazi2010outer}\\
\textcolor{red}{CONVEX}
\begin{itemize}

\item for \emph{convex} MINLPs with separable nonlinear functions
\item propose three improvements to OA algorithms
	\begin{itemize}
	\item extended formulation
	\item refined OA
	\item heuristic inner approximation of feasible region
	\end{itemize}
\item implemented on \texttt{BONMIN}
\end{itemize}

\newpage
\section{neither BB- nor OA-based approaches}
\subsection{The integer approximation error in mixed-integer optimal control [2010, Sager, Bock, Diehl]}
see \cite{sager2012integer}\\
\textcolor{orange}{UNCLEAR}
\begin{itemize}
\item Problem: MINLP that come from discretizing MIOCPs cannot efficiently be solved using nonlinear BnB or outer approximation (due to large number of integer variables)
\item therefore Sager, Reinelt, Bock suggested another approach in "Direct methods with maximal lower bound for mixed-integer optimal control problems" (\cite{sager2009direct}):
	\begin{itemize}
	\item on MIOCPs with swith on/off discrete variables
	\item solve relaxed continuous control problem, get integer solutions by combination of grid adaptivity and Sum Up Rounding Strategy
	\item solves multistage MIOCP
\end{itemize}	 
\item Theorem: solution of relaxed and convexified problem can be approximated arbitrarily close by solution fulfilling the integer requirements
\item discuss Sum Up Rounding Strategy: more than a heuristic (as it was previously described), constructive way to obtain integer solution with bound on performance loss (tolerance depends on control discretization grid)
\item estimate on Hausdorff distance between reachable sets, improvements in approximation order
\item main contribution: complete proof for termination of algorithm suggested by Sager on optimization of switching decisions in nonlinear optimal control
\end{itemize}
\subsection{Non-convex mixed-integer nonlinear programming: A survey [Burer, Letchford, 2012]}
see \cite{burer2012non}\\
\textcolor{green}{NON-CONVEX, Promising}\\
\textbf{What?} Summary of approaches for solving non-convex MINLPs
\begin{itemize}
\item non-convex MINLP are challenging because  continous relaxation in general not convex
\item key concepts forming main ingredients of all existing exact algorithms for non-convex MINLPs
	\begin{itemize}
	\item \emph{Under- and Over-estimators}
		\begin{itemize}
		\item as solving continous relaxation still difficult, need a further relaxation step
		\item replace each non-convex function $f^j(x,y)$ with convex under-estimating function $g^j(x,y)$ (i.e. $g^j(x,y) \leq f^j(x,y)$)
		\item or replace each non-convex function $f^j(x,y)$ by new variable $z^j$ acting as a placeholder and add constraints enforcing that $z^j$ is approximately  $f^j(x,y)$, e.g. $z^j \geq g^j(x,y)$
		\item same works for concave over-estimating functions or use linear under-/overestimating functions when want to use LP solver
		\item for specific functions: convex and concave envelopes (tightest possible convex under-estimator/concave over-estimator)
		\item for examples see references in \cite{burer2012non}, page 99.
		\end{itemize}
	\item \emph{Separable functions}
		\begin{itemize}
		\item function $f(x,y)$ separable if $f(x,y) = \sum_{i=1}^{n_1}g(x_i) + \sum_{j=1}^{n_2}h(y_j)$
		\item sum of individual under-estimators is under-estimator for $f$
		\item approximate non-convex MINLP by MILP by: approximate each individual function with piecewise linear function, introduce continuous variables $g_i, h_i$ representing values of those functions, introduce binary variables for each piece of some linear function, introduce further binary variables with linear constraints to ensure that $g_i, h_i$ take correct values
		\end{itemize}
	\item \emph{Factorization}
		\begin{itemize}
		\item introduce new variables and constraints s.t. resulting MINLP involves functions of simpler form
		\item then find under-/over-estimators for simple functions
		\item see example on page 99 in \cite{burer2012non}
		\end{itemize}
	\item \emph{Branching: standard and spatial}
		\begin{itemize}
		\item standard branching: branch on integer-constrained variable
		\item spatial branching: branch by partitioning domain of continous variables and additionally replace original under- and over-estimators with stronger one, taking advantage of reduced domain
		\end{itemize}
	\end{itemize}
\item algorithms for general case of non-convex MINLPs
	\begin{itemize}
	\item \emph{Spatial branch-and-bound}
		\begin{itemize}
		\item arrange subproblems in tree-structure
		\item prune subproblems if (i) feasible but its cost under relaxed objective equals its true cost, (ii) associated lower bound no better than bst upper bound so far, (ii) infeasible
		\end{itemize}
	\item \emph{Branch-and-reduce}
		\begin{itemize}
		\item improved version of spatial branch-and-bound, attempting to reduce variable domains by more than what results of branching
		\item add two operations:\\ (i) before solving subproblem, check its constraints whther domain of any variable can be reduced without losing any \emph{feasible} solutions;\\ (ii) after solving subproblem, check whether domain of any variable can be reduced without losing any \emph{optimal} solutions
		\item afterwards generate better under-estimator, tighten constraints, possibly improved lower bounds
		\item drastic decrease in size of tree
		\item usally use LP relaxations (rather than convex programming relaxations)
		\end{itemize}
	\item \emph{$\alpha$-branch-and-bound}
		\begin{itemize}
		\item based on general technique for constructing under-estimators
		\item all functions need to be twice differentiable
		\item no additional variables needed, no need of factorization
		\item one needs a general convex programming solver, not only LP solver
		\end{itemize}
	\item \emph{Conversion to MILP}
		\begin{itemize}
		\item factorize problem, approximate resulting separable MINLP by MILP (see above), solve resulting MILP
		\item leads to sets of binary variables with special structure, use specialized branching rule
		\end{itemize}
	\item \emph{Heuristics}
		\begin{itemize}
		\item sometimes possible to convert exact algorithms for convex MINLPs into heuristics for non-convex MINLPs, e.g. see \cite{leyffer2001integrating} or \cite{nannicini2012rounding}
		\end{itemize}
	\item \emph{Other exact algorithms}
		\begin{itemize}
		\item Kesavan \cite{kesavan2004outer}, see \ref{sec:OA_nonconvex}
		\item D'Ambrosio \cite{d2012algorithmic}, see \ref{sec:SQ-MINLP}
		\end{itemize}
	\end{itemize}
\item algorithms for special case of all non-linear functions are quadratic
	\begin{itemize}
	\item describes reformulation-linearization technique
	\end{itemize}
\item Software 
	\begin{itemize}
	\item packages solving non-convex MINLPs to proven optimality:\\
	 \texttt{BARON, $\alpha$-BB, LINDO-Global, Couenne, GloMIQO} 
	\item packages which can be used to find heuristic solutions for non-convex MINLPs: \\
	\texttt{BONMIN, DICOPT, LaGO, MIDACO}
	\end{itemize}	 
\end{itemize}

\subsection{An Algorithmic Framework for MINLP with separable non-convexity [D'Ambrosio, Lee, W\"achter, 2012]}
\label{sec:SQ-MINLP}
see \cite{d2012algorithmic}\\
\textcolor{green}{NON-CONVEX, Promising}
\begin{itemize}
\item suggests simple algorithm using alternating sequence of non-convex NLP restrictions and convex MINLP relaxations
\item for MINLPs with non-convex objective and constraint functions, but sum of non-convex univariate functions (i.e. separable)
\item employ (repeatedly refined) lower bounding \emph{convex} MINLP relaxation by using piecewise-convex under-estimators
\item many problems can be brought in assumed form (i.e. separable) by simple substitutions (similar to what is done in first step in spatial branch-and-bound), see also 3.4 (page 339) in \cite{d2012algorithmic}
\item \textbf{Sequential Convex MINLP algorithm }(implemented in AMPL):
	\begin{itemize}
	\item first, identify convex and concave subintervals (using MATLAB) by computing points at which convexity/concavity may change (i.e. zeros of second derivative) (section 2.1)
	\item then find convex MINLP relaxation (working on these subintervals), handle it using BONMIN solver to find lower bounds
	\item fix integer values and solve non-convex NLP restriction (using IPOPT) to find upper bounds (see section 2.2)
	\item at each step decrease gap between lower and upper bound, use solutions for further refinement following two refinement strategies (see section 2.3)
		\begin{itemize}
		\item based on lower-bounding solution $\underbar{x}$: if $\underbar{x}_k$ lies in concave interval of $g_{ik}$, add $\underbar{x}_k$ as additional breakpoint for relaxation of $g_{ik}$
		\item same based on upper-bounding solution $\bar{x}$
		\end{itemize}
	\end{itemize}
\item \textbf{novelty}: new formulation of convex relaxation: functions approximated only where they are concave, convex parts are taken as they are
\item comparison to spatial branch-and-bound
	\begin{itemize}
	\item refining phase: parts where approximation is bad are improved, but by adding more breakpoints instead of branching!
	\item directly keep multivariate convex functions in relaxations instead of using linear approximations
	\end{itemize}
\item convergence analysis under rather weak assumptions (apart from Lipschitz continuity): either termination at \emph{global solution} or each limit point of lower bound sequence is global solution
\item computational results: 
	\begin{itemize}
	\item for middle sized-problem ($\mathcal{O}(10^2)$) good CPU times (up to two minute), very few iterations needed
	\item most of time spent in solution of small number of convex MINLPs
	\item BONMIN sometimes way faster, but is only a heuristic (when used on non-convex problems)! faster than comparable solvers
\end{itemize}	 
\end{itemize}

\subsection{An algorithmic framework for convex mixed integer nonlinear programs [Bonami et al., 2008]}
see \cite{bonami2008algorithmic}\\
\textcolor{red}{CONVEX}
\begin{itemize}
\item introduces algorithmic framework BONMIN which is exact for convex MINLP, can be used as heuristic for non-convex MINLP
\item use flexible branch-and-cut-scheme, \textbf{combination of BB and OA}
	\begin{itemize}
	\item uses outer approximations \textit{and} subproblem relaxations (relaxing integrality) to compute lower bounds
	\item uses restrictions of integer variables $x\in X \cap \mathbb{Z}^n$ to $x\in \bar{X} \cap \mathbb{Z}^n$ to compute upper bounds
	\item when \emph{only subproblem relaxations} used to compute lower bounds (as solution of NLP) \\
	$\rightarrow$ classical branch-and-bound
	\item when \textit{only outer approximations} used to compute lower bounds (as solution of MILP) \\
	$\rightarrow$ classical OA algorithm
	\end{itemize}
\item implementation:
	\begin{itemize}
	\item \texttt{B-BB}: NLP Branch-and-Bound framework (based on interior-point NLP solver Ipopt)
	\item \texttt{B-OA}: OA framework based on Ipopt, LP solver Clp
	\item \texttt{B-Hyb}: hybrid algorithm obtaining either \texttt{B-BB} or \texttt{B-OA} or anything in between
	\end{itemize}
\item theoretical result: If $T=\{(x^1,y^1), ... ,(x^K,y^K)\}$ any set of points containing suitable points with KKT conditions, $P^{OA}(T)$ OA-relaxation around these points, then $P$ and $P^{OA}(T)$ are equivalent (having same optimal value, corresponding optimal solutions)
\item computational results:
	\begin{itemize}
	\item  \texttt{B-Hyb} dominates \texttt{B-BB} and \texttt{B-OA}
	\item in paper of D'Ambrosio, Lee, W\"achter on "Algorithmic Framework for MINLP with separable \emph{non-convexity}" (see \cite{d2012algorithmic} and \ref{sec:SQ-MINLP}) BONMIN performs very well also as heuristic for non-convex MINLPs
	\end{itemize}
\end{itemize}

\newpage
\section{General papers about MINLPs}
\subsection{Mixed-integer nonlinear optimization [Belotti, Kirches, Leyffer, Majahan, 2013]}
\begin{itemize}
\item models and applications of MINLP
\item state of the art methods
\item both convex and non-convex MINLP
\item non-convex MINLP
\begin{itemize}
\item challenging because even after relaxing the integer variables to continuity the feasible region is in general non-convex, i.e. many local minimma
\item also challenging as linearizations do not yield valid lower bounds any more
\item approaches: piecewise linear approximations, generic strategies for obtaining convex relaxations, spatial branch-and-bound, under-/overestimation of nonconvex functions
\end{itemize}
\item review heuristic techniques obtaining good feasible solutions
\end{itemize}
\subsection{Numerical methods for mixed-integer optimal control problems [Sager, PhD-Thesis, 2005]}
see \cite{sager2005numerical}
\begin{itemize}
\item Chapter 1: MIOCP, Chapter 2: Optimal control, Chapter 3: MINLP
\end{itemize}
\subsection{Fast Numerical Methods for Mixed-Integer Nonlinear Model-Predictive Control [Kirches, PhD-Thesis, 2010]}
\subsection{Mixed integer nonlinear programming tools: an updated practical overview [Ambrosio, 2013]}
see \cite{d2013mixed}
\begin{itemize}
\item difficulties with nonlinear functions and how to use modeling languages to overcome them
	\begin{itemize}
	\item most widely used algebraic modeling languages in MINLP are \texttt{AMPL} and \texttt{GAMS}
	\end{itemize}
\item overview over MILP
\item overview over NLP
\item overview over MINLP
\item overview over MINLP solvers
\end{itemize}

\section{Other algorithms}
\subsection{Rounding-based heuristics for nonconvex MINLPs [Belotti, Nannicini, 2009]}
see \cite{nannicini2009rounding}
\begin{itemize}
\item two primal heuristics for non-convex MINLPs
	\begin{itemize}
	\item feasibility heuristic aiming at finding initial feasible solution
	\item improvement heuristic searching for improved solution in neighborhood of given point
	\end{itemize}
\item basic idea: rounding solution of continous NLP with linear constraints
\item each rounding step accomplished through solving MILP
\item effective in practice
\end{itemize}

\bibliography{../library}{}
\bibliographystyle{plain}
\end{document}
